<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Opracowanie nowych metryk do ewaluacji muzyki symbolicznej w formacie MIDI &mdash; WIMU 10  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=5929fcd5"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  
  
    
  
    
  
  
  
  
   
  <link rel="preload" href="../_static/css/fonts/fontawesome-webfont.woff2?af7ae505a9eed503f8b8e6982036873e" as="font">
  <link rel="stylesheet" href="../_static/css/index.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/dark.css" media="(prefers-color-scheme: dark)" />
  <script src="../_static/js/scheme-switcher.js"></script>

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            WIMU 10
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Informacje Ogólne</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../LICENSE.html">Licencja</a></li>
<li class="toctree-l1"><a class="reference internal" href="../DESIGN_PROPOSAL.html">Design Proposal</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Dokumentacja API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api/metrics.html">Metryki</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/utils.html">Narzędzia</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Eksperymenty</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../research/chords-metrics.html">Miary oparte o akordy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../research/dynamics-metrics.html">Miary oparte o dynamikę</a></li>
<li class="toctree-l1"><a class="reference internal" href="../research/embeddings.html">Embeddingi CLaMP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../research/self_similarity.html">Samopodobieństwo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../research/tokenization.html">Tokenizacja</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Nowe eksperymenty</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../research/key-metrics.html">Miary oparte o tonacje</a></li>
<li class="toctree-l1"><a class="reference internal" href="../research/time-signature-metrics.html">Miary oparte o tempo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../research/syncope-metrics.html">Miary oparte o synkopę</a></li>
<li class="toctree-l1"><a class="reference internal" href="../research/clamp-tests.html">Nowe testy CLaMP</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">WIMU 10</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Opracowanie nowych metryk do ewaluacji muzyki symbolicznej w formacie MIDI</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/old/DESIGN_PROPOSAL.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="opracowanie-nowych-metryk-do-ewaluacji-muzyki-symbolicznej-w-formacie-midi">
<h1>Opracowanie nowych metryk do ewaluacji muzyki symbolicznej w formacie MIDI<a class="headerlink" href="#opracowanie-nowych-metryk-do-ewaluacji-muzyki-symbolicznej-w-formacie-midi" title="Link to this heading"></a></h1>
<p>Temat realizowany przez zespół nr 10 w składzie:</p>
<ul class="simple">
<li><p>Moroz Bartłomiej,</p></li>
<li><p>Motyka Jakub,</p></li>
<li><p>Sygocki Dawid,</p></li>
<li><p>Walczak Jan.</p></li>
</ul>
<section id="planowana-funkcjonalnosc-programu">
<h2>Planowana funkcjonalność programu<a class="headerlink" href="#planowana-funkcjonalnosc-programu" title="Link to this heading"></a></h2>
<p>Program ma przyjąć formę otwartoźródłowej biblioteki do języka Python.
Decyzję taką podjęliśmy ze względu na potencjalne wykorzystanie w kodzie modeli uczenia maszynowego. Integracja takiego kodu z narzędziem wyłącznie konsolowym byłaby niepraktyczna.
API biblioteki składać się będzie z zestawu funkcji do oceny jakości muzyki w formacie MIDI <a class="footnote-reference brackets" href="#back1999" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>.</p>
<p>Repozytorium zawierać będzie również przykładowy program (skrypt) konsolowy prezentujący sposób korzystania z API.
Program pozwalał będzie na wyznaczenie metryk dla wybranych plików w trybie wsadowym.</p>
<section id="oferowane-metryki-propozycje-pomysly">
<h3>Oferowane metryki (propozycje, pomysły)<a class="headerlink" href="#oferowane-metryki-propozycje-pomysly" title="Link to this heading"></a></h3>
<ol class="arabic simple">
<li><p>Jednym z kierunków badania będzie wykorzystanie łańcuchów Markova do oceny powtarzalności <a class="footnote-reference brackets" href="#dai2022" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a> wewnątrz utworu.
Zainspirujemy się analizą łańcuchów DNA, aby uwzględnić przerwy pomiędzy fragmentami utworu oraz wariacje takie jak wprowadzenie ozdobników, nut pośrednich, rozdrobnienie rytmu, itp.
Łańcuchy Markova zostały już wykorzystane w dziedzinie MIR przy uczeniu i ocenianiu modelu <a class="footnote-reference brackets" href="#chi2020" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a> poprzez próbkowanie Gibbsa.
Samopodobieństwo możemy sprawdzać pod względem:</p>
<ul class="simple">
<li><p><em>melodycznym</em> - ogólna powtarzalność wysokości i rytmu dźwięków, szczególnie w sekcji perkusyjnej,</p></li>
<li><p><em>dynamicznym</em> - spójność głośności na przestrzeni utworu:</p>
<ul>
<li><p>ta sama ogólna głośność przez cały utwór lub przynajmniej w jego sekcjach,</p></li>
<li><p>powtarzalność schematów zmiany głośności w powiązaniu z elementami utworu (np. pod koniec każdego zdania),</p></li>
</ul>
</li>
<li><p><em>agogicznym</em> - spójność tempa, jw.,</p></li>
<li><p><em>harmonicznym</em> - podobieństwo pochodów akordów (po “skwantyzowaniu” melodii do akordów); można uwzględnić zmiany klucza i badać akordy nie bezwględnie, ale jako stopnie tonacji,</p></li>
</ul>
</li>
<li><p>Zauważyliśmy, że zbiór statystycznych metryk obecnych w MusPy <a class="footnote-reference brackets" href="#dong2020" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#yang2020" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a> jest niewielkich rozmiarów i można go potencjalnie rozszerzyć. Sugerując się m.in. <a class="footnote-reference brackets" href="#ji2020" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#xiong2023" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a>, proponujemy wstępnie następujące miary:</p>
<ul class="simple">
<li><p><em>wysokościowa/harmoniczna</em> - histogram klas akordów występujących w formie melodycznej (sekwencyjnie) i harmonicznej (wielogłosowo, jako współbrzmienie);
trudność polega na dopasowywaniu akordów zależnie od przyjętej granularności
(czy bierzemy pod uwagę strukturę utworu i dopasowujemy akord na takt lub miarę, czy może przetwarzamy sekwencję w następujących po sobie grupach nutowych niezależnie od ich wartości rytmicznej i umiejscowienia),</p></li>
<li><p><em>wysokościowa/harmoniczna</em> - macierz przejść między klasami akordów na podstawie miary wyżej opisanej, analogicznie do istniejącej już w MusPy metryki histogramu wysokości i macierzy przejść między wysokościami;
dopasowane klasy akordów nie muszą jednak bezpośrednio sąsiadować - mogą być rozdzielone pojedynczymi nutami (problem taki nie występuje we wspomnianej metryce z MusPy),</p></li>
<li><p><em>rytmiczna</em> - podsumowanie liczby synkop (przesunięć akcentu na “słabą” część taktu),
potencjalnie również w formie histogramu, bo istnieje ograniczona liczba wzorców (np. szesnastka + ósemka + szesnastka),</p></li>
<li><p><em>rytmiczna</em> - alternatywa do miary poprzedniej, czyli ile razy dłuższa nuta przypada na “silną” część taktu,</p></li>
<li><p><em>wysokościowa/harmoniczna</em> liczba zmian tonacji;
wymaga wpierw wykrycia tonacji - można się wspomóc metazdarzeniem zmiany znaków przykluczowych (<code class="docutils literal notranslate"><span class="pre">FF</span> <span class="pre">59</span> <span class="pre">02</span></code>) zdefiniowanym w standardzie MIDI, ale nie jest to równoważne ani nawet jednoznaczne,</p></li>
<li><p><em>dynamiczna</em> - liczba zmian głośności: stopniowych (crescendo, decresendo; sąsiedztwo głośności zbliżonych, np. p -&gt; mp) oraz nagłych (sąsiedztwo głośności skrajnych, np. p -&gt; f; akcenty),
z podziałem na instrumenty lub całościowo dla utworu;
podział na sekcje o danej głośności utrudnia fakt, że każde zdarzenie Note On definiuje ją osobno,</p></li>
<li><p><em>agogiczna</em> - liczba zmian w tempie (metazdarzenie <code class="docutils literal notranslate"><span class="pre">FF</span> <span class="pre">51</span> <span class="pre">03</span></code>);
na poziomie MIDI “ogólne” zmiany tempa są nieodróżnialne od chwilowych przyspieszeń lub spowolnień (accel., rit., fermata), ale można próbować je sklasyfikować ze względu na długość oraz umiejscowienie,</p></li>
<li><p><em>rytmiczna</em> - liczba zmian metrum (metazdarzenie <code class="docutils literal notranslate"><span class="pre">FF</span> <span class="pre">58</span> <span class="pre">04</span></code>),
pytaniem pozostaje, czy istnieją modele generatywne, które zmiany metrum wspierają.</p></li>
</ul>
</li>
</ol>
<p>Opisane wyżej metryki można wyliczyć dla istniejących utworów reprezentujacych różne gatunki (niżej opisany zbiór danych nr 1) i na podstawie wyników próbować klasyfikować utwory wygenerowane (zbiór danych nr 2).
Podobnie jak w algorytmie “Inception Score”, wysoka niejednoznaczność klasyfikacji pojedynczego utworu mogłaby oznaczać słabą “ostrość” reprezentowanego przez niego gatunku, a więc słabą jakość.
Uważać trzeba na przypadki szczególne - mieszanie gatunków w obrębie pojedynczego utworu, gatunki pośrednie lub niedookreślone.</p>
<p>Ponadto, możemy próbować przekształcić powyższe metryki na dwustopniowe lub trzystopniowe miary subiektywne (jakość dobra - zła oraz brak wpływu metryki na ocenę).
Jesteśmy w stanie dokonać badań statystycznych polegających na ocenie wycinka zbiorów danych w obrębie naszego zespołu.
Badania takie miałyby na celu określenie akceptowalnych i nieakceptowalnych zakresów wartości wyznaczonych metryk.</p>
</section>
</section>
<section id="zbiory-danych">
<h2>Zbiory danych<a class="headerlink" href="#zbiory-danych" title="Link to this heading"></a></h2>
<p>Zgrubny podział zbiorów danych:</p>
<ol class="arabic simple" start="0">
<li><p>ręcznie spreparowane minimalne dane testowe do sprawdzania poprawności działania metryk,</p></li>
<li><p>istniejące utwory w MIDI (podzbiór katalogu oferowanego w ramach MusPy),</p></li>
<li><p>utwory w MIDI wygenerowane z użyciem istniejących, wytrenowanych modeli uczenia maszynowego (technologia GAN <a class="footnote-reference brackets" href="#dong2017" id="id8" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a> lub Transformer <a class="footnote-reference brackets" href="#huang2018" id="id9" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a>).</p></li>
</ol>
<p>Dla obu zbiorów wyznaczymy wartości metryk opracowanych przez nas, jak i obecnych już w MusPy (celem porównania).</p>
<section id="analiza-wybranych-zbiorow-danych-dostepnych-w-muspy">
<h3>Analiza wybranych zbiorów danych dostępnych w MusPy<a class="headerlink" href="#analiza-wybranych-zbiorow-danych-dostepnych-w-muspy" title="Link to this heading"></a></h3>
<p>Przy analizie zbiorów danych ustaliliśmy następujące ograniczenia:</p>
<ul class="simple">
<li><p>zbiory muszą być dostępne poprzez interfejs MusPy,</p></li>
<li><p>interesują nas tylko utwory w formacie MIDI,</p></li>
<li><p>nie uwzględniamy chorałów i hymnów.</p></li>
</ul>
<p>W ten sposób otrzymaliśmy listę pięciu zbiorów danych nadających się do naszego zadania, których analizę zamieszczamy poniżej. Zagadnienia, które uznaliśmy za ważne, to:</p>
<ul class="simple">
<li><p>jakość plików MIDI/transkrypcji,</p></li>
<li><p>sposób pozyskania danych,</p></li>
<li><p>rozmiar zbioru danych,</p></li>
<li><p>gatunek muzyki,</p></li>
<li><p>liczba instrumentów/kanałów,</p></li>
<li><p>ewentualna potrzeba dodatkowego przetwarzania,</p></li>
<li><p>popularność/wykorzystanie w innych pracach.</p></li>
</ul>
<p>Wybrane zbiory danych:</p>
<ul class="simple">
<li><p>The Lakh MIDI Dataset <a class="footnote-reference brackets" href="#raffel2016dataset" id="id10" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#raffel2016" id="id11" role="doc-noteref"><span class="fn-bracket">[</span>11<span class="fn-bracket">]</span></a> - Największy z dostępnych zbiorów danych. Zawiera 176 581 różnych utworów w formacie MIDI (&gt;5000h), 45 tys. z których występuje również w zestawie danych “Million Song Dataset” <a class="footnote-reference brackets" href="#bertin-mahieux2011" id="id12" role="doc-noteref"><span class="fn-bracket">[</span>12<span class="fn-bracket">]</span></a>. Zbiór zawiera wyłącznie pliki pobrane z Internetu z publicznie dostępnych źródeł bez zachowania kontroli jakości. W konsekwencji w zbiorze znajdują się także pliki o niepoprawnym formacie, wyjątkowo krótkie lub niskiej jakości. Na podstawie tego zestawu danych powstał zestaw “Lakh Pianoroll Dataset” <a class="footnote-reference brackets" href="#dong2017dataset" id="id13" role="doc-noteref"><span class="fn-bracket">[</span>13<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#dong2017" id="id14" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a>, który po oczyszczeniu zawiera 174 154 plików. Ten zestaw został później wykorzystany w wytrenowaniu modelu MuseGAN <a class="footnote-reference brackets" href="#dong2017" id="id15" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a>.</p></li>
<li><p>MAESTRO <a class="footnote-reference brackets" href="#hawthorne2019" id="id16" role="doc-noteref"><span class="fn-bracket">[</span>14<span class="fn-bracket">]</span></a> - Zestaw zawierający 1282 utworów (~201h) muzyki klasycznej na fortepian. Udostępnia zarówno pliki w formacie MIDI, jak również ich odpowiedniki w formacie WAV oraz metadane w formacie CSV i JSON zawierające informacje o artystach, czasy trwania oraz nazwy utworów. Zbiór powstał przy współpracy z organizatorami konkursu “International Piano-e-Competition”, podczas którego wszystkie utwory były wykonywane na fortepianach Yamaha Disklavier nagrywających MIDI. Autorzy nie podają informacji o procedurach przygotowania danych do użycia, a artykuły wykorzystujące MAESTRO <a class="footnote-reference brackets" href="#bittner2022" id="id17" role="doc-noteref"><span class="fn-bracket">[</span>15<span class="fn-bracket">]</span></a> nie wskazują, by jakiekolwiek przetwarzanie wstępne było potrzebne do pracy z zestawem.</p></li>
<li><p>NES-MDB (Nintendo Entertainment System Music Database) <a class="footnote-reference brackets" href="#donahue2018" id="id18" role="doc-noteref"><span class="fn-bracket">[</span>16<span class="fn-bracket">]</span></a> - Zestaw składający się z 5278 różnych utworów (~46h) zebranych z 397 gier na konsolę NES (Nintendo Entertainment System). Pliki MIDI zostały otrzymane przez konwersję z natywnego dla NES formatu muzyki (instrukcji wbudowanego czterokanałowego syntezatora dźwięku). W porównaniu z pozostałymi zestawami danych, bardzo mała ilość artykułów wykorzystuje NES-MDB.</p></li>
<li><p>MusicNet <a class="footnote-reference brackets" href="#thickstun2017" id="id19" role="doc-noteref"><span class="fn-bracket">[</span>17<span class="fn-bracket">]</span></a> - Zbiór 330 wieloinstrumentalnych utworów muzyki klasycznej (~30h) zawierający pliki MIDI, WAV, CSV (utwory w postaci etykiet nut) i metadane. W skład wchodzą utwory następujących kompozytorów: Bach, Beethoven, Faure, Brahms, Haydn, Cambini, Dvorak, Mozart oraz Schubert. Pliki MIDI to publicznie dostępne, ręcznie wykonane transkrypcje zapisu nutowego. W artykule “MT3: Multi-Task Multitrack Music Transcription” <a class="footnote-reference brackets" href="#gardner2022" id="id20" role="doc-noteref"><span class="fn-bracket">[</span>18<span class="fn-bracket">]</span></a> w zestawieniu z kilkoma innymi zestawami danych (MAESTROv3 <a class="footnote-reference brackets" href="#hawthorne2019" id="id21" role="doc-noteref"><span class="fn-bracket">[</span>14<span class="fn-bracket">]</span></a>, Slakh2100 <a class="footnote-reference brackets" href="#manilow2019" id="id22" role="doc-noteref"><span class="fn-bracket">[</span>19<span class="fn-bracket">]</span></a>, Cerberus4 <a class="footnote-reference brackets" href="#manilow2020" id="id23" role="doc-noteref"><span class="fn-bracket">[</span>20<span class="fn-bracket">]</span></a>, GuitarSet <a class="footnote-reference brackets" href="#xi2018" id="id24" role="doc-noteref"><span class="fn-bracket">[</span>21<span class="fn-bracket">]</span></a>, URMP <a class="footnote-reference brackets" href="#li2018" id="id25" role="doc-noteref"><span class="fn-bracket">[</span>22<span class="fn-bracket">]</span></a>), model do transkrypcji muzyki wytrenowany na MusicNet osiągnął najniższe wyniki. Autorzy artykułu tłumaczyli to niewielkim rozmiarem materiału uczącego i złym wyrównaniem w czasie dźwięków z nagrań i odpowiadających im etykiet. Druga z wymienionych wad nie ma dla nas znaczenia, ponieważ operujemy wyłącznie na reprezentacji symbolicznej.</p></li>
<li><p>EMOPIA (A Multi-Modal Pop Piano Dataset For Emotion Recognition and Emotion-based Music Generation) <a class="footnote-reference brackets" href="#hung2021" id="id26" role="doc-noteref"><span class="fn-bracket">[</span>23<span class="fn-bracket">]</span></a> - Zestawienie 1087 fragmentów 387 utworów (~11h), skupione na postrzeganiu emocji w muzyce popowej/filmowej na pianino. Wszystkie fragmenty zostały przydzielone do jednej z czterech grup na podstawie ich pozytywności (“valence”) oraz pobudzenia (“arousal”). Transkrypcję z audio do MIDI otrzymano przy wykorzystaniu modelu AI <a class="footnote-reference brackets" href="#kong2020" id="id27" role="doc-noteref"><span class="fn-bracket">[</span>24<span class="fn-bracket">]</span></a>.</p></li>
</ul>
</section>
</section>
<section id="planowany-zakres-eksperymentow">
<h2>Planowany zakres eksperymentów<a class="headerlink" href="#planowany-zakres-eksperymentow" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p>Zbadamy graniczne oraz umiarkowane wartości niektórych metryk poprzez spreparowanie plików MIDI o odpowiedniej charakterystyce.
W przypadku łańcuchów Markova zależy nam na wartościach dla braku powtarzalności, stuprocentowej powtarzalności oraz umiarkowanej powtarzalności w postaci znanych form okresowych utworów (ABA, ABA’, AABA, ABAC, itd.)</p></li>
<li><p>Dla wybranych zbiorów danych porównamy metryki z MusPy z naszymi.
Sprawdzimy, czy są od siebie zależne, tzn. czy kierunek zmiany obu metryk jest skorelowany.</p></li>
<li><p>(opcjonalnie) Zbadamy, jak mają się wartości metryk na wybranych utworach do naszych odczuć.
Zrobimy to w sposób porównawczy, prezentując badanej osobie dwa utwory o różnych wartościach metryk.
Potencjalnie wykorzystamy do oceny skalę Likerta z ogólnymi stwierdzeniami postaci “Utwór nr 1 brzmi lepiej niż utwór nr 2”.</p></li>
</ol>
</section>
<section id="planowany-stack-technologiczny">
<h2>Planowany stack technologiczny<a class="headerlink" href="#planowany-stack-technologiczny" title="Link to this heading"></a></h2>
<p>Język programowania: Python
Język opisu: Markdown (MyST)</p>
<p>Docelowy format: biblioteka.</p>
<p>Narzędzia:</p>
<ul class="simple">
<li><p>środowisko wirtualne - venv,</p></li>
<li><p>linter, autoformatowanie - ruff,</p></li>
<li><p>przetwarzanie - NumPy, MusPy,</p></li>
<li><p>testowanie - pytest,</p></li>
<li><p>dokumentacja - Sphinx,</p></li>
<li><p>interfejs konsolowy - argparse,</p></li>
<li><p>rejestrowanie zdarzeń - logging,</p></li>
<li><p>wizualizacja - matplotlib.</p></li>
</ul>
</section>
<section id="harmonogram-i-planowany-postep">
<h2>Harmonogram i planowany postęp<a class="headerlink" href="#harmonogram-i-planowany-postep" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>23.10 - 27.10 - etap 1: zgłoszenie propozycji rozwiązania, wstępna ogólna konfiguracja środowiska,</p></li>
<li><p>30.10 - 03.11<br />
06.11 - 10.11<br />
13.11 - 17.11
20.11 - 24.11 - etap 2: postęp analizy literaturowej, konfiguracja środowiska eksperymentalnego,</p></li>
<li><p>27.11 - 01.12<br />
04.12 - 08.12<br />
11.12 - 15.12 - implementacja metryk, wypróbowanie metryk dla zestawów danych,</p></li>
<li><p>18.12 - 22.12<br />
03.01 - 05.01 - analiza wyników metryk oraz ich porównanie wraz z wynikami innych metryk,</p></li>
<li><p>08.01 - 12.01<br />
15.01 - 19.01 - sprawdzenie potencjalnych współzależności między metrykami naszymi a już istniejącymi,</p></li>
<li><p>22.01 - 26.01 - etap finałowy</p></li>
</ul>
</section>
<section id="bibliografia">
<h2>Bibliografia<a class="headerlink" href="#bibliografia" title="Link to this heading"></a></h2>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="back1999" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://www.music.mcgill.ca/~ich/classes/mumt306/StandardMIDIfileformat.html">“Standard MIDI-File Format Spec. 1.1, updated”, David Back, 1999</a></p>
</aside>
<aside class="footnote brackets" id="dai2022" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://arxiv.org/abs/2209.00182">“What is missing in deep music generation? A study of repetition and structure in popular music”, Shuqi Dai &amp; Huiran Yu &amp; Roger B. Dannenberg, 2022</a></p>
</aside>
<aside class="footnote brackets" id="chi2020" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">3</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://arxiv.org/abs/2008.08927">“Generating Music with a Self-Correcting Non-Chronological Autoregressive Model”, Wayne Chi et al., 2020</a></p>
</aside>
<aside class="footnote brackets" id="dong2020" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">4</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://arxiv.org/abs/2008.01951">“MusPy: A toolkit for symbolic music generation”, Hao-Wen Dong et al., 2020</a></p>
</aside>
<aside class="footnote brackets" id="yang2020" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">5</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://www.researchgate.net/publication/328728367_On_the_evaluation_of_generative_models_in_music">“On the evaluation of generative models in music”, Li-Chia Yang &amp; Alexander Lerch, 2020</a></p>
</aside>
<aside class="footnote brackets" id="ji2020" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">6</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://arxiv.org/abs/2011.06801">“A Comprehensive Survey on Deep Music Generation: Multi-level Representations, Algorithms, Evaluations, and Future Directions”, Shulei Ji &amp; Jing Luo &amp; Xinyu Yang, 2020</a></p>
</aside>
<aside class="footnote brackets" id="xiong2023" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">7</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://arxiv.org/abs/2308.13736">“A Comprehensive Survey for Evaluation Methodologies of AI-Generated Music”, Zeyu Xiong et al., 2023</a></p>
</aside>
<aside class="footnote brackets" id="dong2017" role="note">
<span class="label"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id8">1</a>,<a role="doc-backlink" href="#id14">2</a>,<a role="doc-backlink" href="#id15">3</a>)</span>
<p><a class="reference external" href="https://arxiv.org/abs/1709.06298">“MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment”, Hao-Wen Dong et al., 2017</a></p>
</aside>
<aside class="footnote brackets" id="huang2018" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">9</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://arxiv.org/abs/1809.04281">“Music Transformer: Generating music with long-term structure”, Cheng-Zhi Anna Huang et al., 2018</a></p>
</aside>
<aside class="footnote brackets" id="raffel2016dataset" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">10</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://colinraffel.com/projects/lmd/">“The Lakh MIDI Dataset”, Collin Raffel, accessed 14.11.2023</a></p>
</aside>
<aside class="footnote brackets" id="raffel2016" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">11</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://academiccommons.columbia.edu/doi/10.7916/D8N58MHV">“Learning-Based Methods for Comparing Sequences, with Applications to Audio-to-MIDI Alignment and Matching”, Collin Raffel, 2016</a></p>
</aside>
<aside class="footnote brackets" id="bertin-mahieux2011" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">12</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://academiccommons.columbia.edu/doi/10.7916/D8NZ8J07">“The Million Song Dataset”, Thierry Bertin-Mahieux, 2011</a></p>
</aside>
<aside class="footnote brackets" id="dong2017dataset" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id13">13</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://salu133445.github.io/lakh-pianoroll-dataset/">“Lakh Pianoroll Dataset”, Hao-Wen Dong et al., accessed 17.11.2023</a></p>
</aside>
<aside class="footnote brackets" id="hawthorne2019" role="note">
<span class="label"><span class="fn-bracket">[</span>14<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id16">1</a>,<a role="doc-backlink" href="#id21">2</a>)</span>
<p><a class="reference external" href="https://openreview.net/forum?id=r1lYRjC9F7">“Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset”, Curtis Hawthorne et al., 2019</a></p>
</aside>
<aside class="footnote brackets" id="bittner2022" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id17">15</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://arxiv.org/abs/2203.09893">“A Lightweight Instrument-Agnostic Model for Polyphonic Note Transcription and Multipitch Estimation”, Rachel M. Bittner et al., 2022</a></p>
</aside>
<aside class="footnote brackets" id="donahue2018" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id18">16</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://arxiv.org/abs/1806.04278">“The NES Music Database: A multi-instrumental dataset with expressive performance attributes”, Donahue et al., 2018</a></p>
</aside>
<aside class="footnote brackets" id="thickstun2017" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id19">17</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://arxiv.org/abs/1611.09827">“Learning Features of Music from Scratch”, John Thickstun et al., 2017</a></p>
</aside>
<aside class="footnote brackets" id="gardner2022" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id20">18</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://arxiv.org/abs/2111.03017v4">“MT3: Multi-Task Multitrack Music Transcription”, Josh Gardner et al., 2022</a></p>
</aside>
<aside class="footnote brackets" id="manilow2019" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id22">19</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://arxiv.org/abs/1909.08494">“Cutting Music Source Separation Some Slakh: A Dataset to Study the Impact of Training Data Quality and Quantity”, Ethan Manilow et al., 2019</a></p>
</aside>
<aside class="footnote brackets" id="manilow2020" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id23">20</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://arxiv.org/abs/1910.12621">“Simultaneous Separation and Transcription of Mixtures with Multiple Polyphonic and Percussive Instruments”, Ethan Manilow et al., 2020</a></p>
</aside>
<aside class="footnote brackets" id="xi2018" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id24">21</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://archives.ismir.net/ismir2018/paper/000188.pdf">Guitarset: A Dataset for Guitar Transcription, Qingyang Xi et al., 2018</a></p>
</aside>
<aside class="footnote brackets" id="li2018" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id25">22</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://labsites.rochester.edu/air/publications/li2018creating.pdf">“Creating a multi-track classical music performance dataset for multi-modal music analysis: Challenges, insights, and applications”, Bochen Li et al., 2018</a></p>
</aside>
<aside class="footnote brackets" id="hung2021" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id26">23</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://arxiv.org/abs/2108.01374">“EMOPIA: A Multi-Modal Pop Piano Dataset For Emotion Recognition and Emotion-based Music Generation”, Hsiao-Tzu Hung et al., 2021</a></p>
</aside>
<aside class="footnote brackets" id="kong2020" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id27">24</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://arxiv.org/abs/2010.01815">“High-resolution Piano Transcription with Pedals by Regressing Onset and Offset Times”, Qiuqiang Kong et al., 2020</a></p>
</aside>
</aside>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, WIMU 10 Contributors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <script>createSchemeSwitcher()</script>


</body>
</html>